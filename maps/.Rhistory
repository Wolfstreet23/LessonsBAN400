)
# Start the experiments. First single CPU everywhere:
tic("Single CPU on all calculations")
for (i in 1:nrow(df)) {
df$share_reject[i] <-
MTweedieTests(N = df$N[i],
M = df$M[i],
sig = .05)
}
toc(log = TRUE)
# Next we use doParallel for the loop over the result df:
maxcores <- 8
Cores <- min(parallel::detectCores(), maxcores)
cl <- makeCluster(Cores)
registerDoParallel(cl)
# Take the time as before:
tic(paste0("Parallel loop, ", Cores, " cores"))
res <-
foreach(
i = 1:nrow(df),
.combine = 'rbind',
.packages = c('magrittr', 'dplyr', 'tweedie')
) %dopar%
tibble(
N = df$N[i],
M = df$M[i],
share_reject =
MTweedieTests(N = df$N[i],
M = df$M[i],
sig = .05)
)
# Now that we're done, we close off the clusters
stopCluster(cl)
toc(log = TRUE)
# Finally a regular loop over the result df,
# but parallel version of the simulation function:
tic("Parallel repeat")
for (i in 1:nrow(df)) {
df$share_reject[i] <-
MParTweedieTests(
N = df$N[i],
M = df$M[i],
sig = .05,
maxcores = 8
)
}
toc(log = TRUE)
# You can see below that parallelizing the mMTweedieTests-function
# speeds things up much more than parallelizing the final loop.
#
# In this case, the replicate(M,simTweedieTest(N)) is just very
# demanding, and particularly when M is large. Therefore,
# parallelizing this part leads to speed gains.
#
# Parallelizing the final loop doesn't do very much. We only
# have five different calls to mMTweedieTests, so having
# more than five cores doesn't help at all. And some of
# the calls (e.g. mMTweedieTests when M=10) are very fast.
# This means that the core assigned to M=10 will finish
# early, and the rest will wait for the M=5000 to finish.
#
# If we instead wanted to call mMTweedieTests many times
# with a low M, the results might be reversed!
tic.log() %>%
unlist %>%
tibble(logvals = .) %>%
separate(logvals,
sep = ":",
into = c("Function type", "log")) %>%
mutate(log = str_trim(log)) %>%
separate(log,
sep = " ",
into = c("Seconds"),
extra = "drop")
# Start the experiments. First single CPU everywhere:
tic("Single CPU on all calculations")
for (i in 1:nrow(df)) {
df$share_reject[i] <-
MTweedieTests(N = df$N[i],
M = df$M[i],
sig = .05)
}
toc(log = TRUE)
# Start the experiments. First single CPU everywhere:
tic("Single CPU on all calculations")
for (i in 1:nrow(df)) {
df$share_reject[i] <-
MTweedieTests(N = df$N[i],
M = df$M[i],
sig = .05)
}
toc(log = TRUE)
# Next we use doParallel for the loop over the result df:
maxcores <- 8
Cores <- min(parallel::detectCores(), maxcores)
cl <- makeCluster(Cores)
registerDoParallel(cl)
# Take the time as before:
tic(paste0("Parallel loop, ", Cores, " cores"))
res <-
foreach(
i = 1:nrow(df),
.combine = 'rbind',
.packages = c('magrittr', 'dplyr', 'tweedie')
) %dopar%
tibble(
N = df$N[i],
M = df$M[i],
share_reject =
MTweedieTests(N = df$N[i],
M = df$M[i],
sig = .05)
)
# Now that we're done, we close off the clusters
stopCluster(cl)
toc(log = TRUE)
# Finally a regular loop over the result df,
# but parallel version of the simulation function:
tic("Parallel repeat")
for (i in 1:nrow(df)) {
df$share_reject[i] <-
MParTweedieTests(
N = df$N[i],
M = df$M[i],
sig = .05,
maxcores = 8
)
}
toc(log = TRUE)
tic.log() %>%
unlist %>%
tibble(logvals = .) %>%
separate(logvals,
sep = ":",
into = c("Function type", "log")) %>%
mutate(log = str_trim(log)) %>%
separate(log,
sep = " ",
into = c("Seconds"),
extra = "drop")
library(dplyr)
library(magrittr)
df <-
tibble(x = rnorm(100)*100) %>%
mutate(y = 5 + x + rnorm(100))
preprocessData <-
function(df) {
df$x_scaled = df$x/100
return(df)
}
model <-
df %>%
preprocessData() %>%
lm(y~x, data=.)
saveRDS(model, "model.rds")
saveRDS(preprocessData, "preprocess_data.rds")
?Rmarkdown
?RMarkdown
?RMD
?Rmd
install.packages("rmarkdown")
install.packages("formatR")
library(formatR)
```{r eval = TRUE, echo = TRUE}
oddCheck <- function(x) {
# Function for checking whether value given as argument x is an odd number, if so it returns TRUE.
if (x %% 2 == 0) {  # The number is even, which returns false.
return(FALSE)
}
if (x %% 2 != 0) {  # The number is odd, which returns true.
return(TRUE)
}
}
```
oddCheck <- function(x) {
# Function for checking whether value given as argument x is an odd number, if so it returns TRUE.
if (x %% 2 == 0) {  # The number is even, which returns false.
return(FALSE)
}
if (x %% 2 != 0) {  # The number is odd, which returns true.
return(TRUE)
}
}
Quadratic <- function(T) {
# Function: Checks whether values in vector given as argument T is odd.
# If so, the odd values are multiplied by the value 4.
for (row in 1:nrow(T)) {  # For-looping over all rows in vector T.
for(col in 1:ncol(T)) { # For-looping over all columns in vector T.
if (oddCheck(T[row,col])) { # Checks if value t[row, col] is odd, e.g checking all values in "T" for odd numbers.
T[row,col] <- T[row,col] * 4 # Replaces T[row, col] with values from T[row,col] * 4.
}
}
}
return(T) # Returns the desired matrix with the modified values which is that all odd numbers are multiplied by 4.
}
library(readxl)
install.packages("readxl")
setwd("C:/Users/EirikHjalte/OneDrive - AVA of Norway AS/Desktop/Statistics")
library(readxl)
SIN<- read_excel("EveryExistingSupplierInvoice.xlsx") #All suppliers invoices ever recorded in Rambase
knitr::opts_chunk$set(echo = FALSE, eval = TRUE)
SIN<- read_excel("EveryExistingSupplierInvoice.xlsx")
setwd("C:/Users/EirikHjalte/OneDrive - AVA of Norway AS/Desktop/Statistics")
getwd()
install.packages("readxl")
install.packages("readxl")
library(readxl)
knitr::opts_chunk$set(echo = FALSE, eval = TRUE)
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
setwd("C:/Users/EirikHjalte/iCloudDrive/Høst 2021/BAN400/Lessons/parallel_computing")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
###Renaming our columns for simplicity, and readability.
```{r eval = TRUE, echo = TRUE}
SIN <- rename(SIN, ST = `St`, ID = `Id`, Date = `Created at`, QTY = `Quantity: quantity`, Unit = `Quantity: Unit`,
ART = `Product: Name`, Name = `Product: productDescription`, RegistrationDate = `Registration date`,
SupRefNumber = `Supplier ref  no`, NetPrice_NOK = `Net price /NOK`, NetAmount_NOK = `Net amount /NOK`)
#Viewing our df in spreadsheet like display
View(SIN)
```
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
getwd()
head(SIN) #Viewing the head of our df
library(readxl)
setwd("C:/Users/EirikHjalte/iCloudDrive/Høst 2021/BAN400/Lessons/parallel_computing")
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
library(tidyverse)
library(readr)
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
library(ggrepel)
library(readxl)
library(ggplot2)
library(tidyverse)
library(readr)
library(scales)
library(readxl)
```{r eval = TRUE, echo = TRUE}
SIN <- read_excel("EveryExistingSupplierInvoice.xlsx")
library(ggrepel)
library(readxl)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(readr)
library(scales)
###Renaming our columns for simplicity, and readability.
```{r eval = TRUE, echo = TRUE}
SIN <- rename(SIN, ST = `St`, ID = `Id`, Date = `Created at`, QTY = `Quantity: quantity`, Unit = `Quantity: Unit`,
ART = `Product: Name`, Name = `Product: productDescription`, RegistrationDate = `Registration date`,
SupRefNumber = `Supplier ref  no`, NetPrice_NOK = `Net price /NOK`, NetAmount_NOK = `Net amount /NOK`)
#Viewing our df in spreadsheet like display
View(SIN)
```
###Renaming our columns for simplicity, and readability.
```{r eval = TRUE, echo = TRUE}
library(dplyr)
SIN <- rename(SIN, ST = `St`, ID = `Id`, Date = `Created at`, QTY = `Quantity: quantity`, Unit = `Quantity: Unit`,
ART = `Product: Name`, Name = `Product: productDescription`, RegistrationDate = `Registration date`,
SupRefNumber = `Supplier ref  no`, NetPrice_NOK = `Net price /NOK`, NetAmount_NOK = `Net amount /NOK`)
#Viewing our df in spreadsheet like display
View(SIN)
```
setwd("C:/Users/EirikHjalte/iCloudDrive/Høst 2021/BAN400/RMarkdown Template")
getwd()
# Exercises for the many models lesson.
#
# 1. (Basic) Exercises 1-3 under 25.2.5 in R4DS
# 2. (Basic-ish) The Jackknife problem from early BAN420.
# 3. (Challenging) The all-combinations problem from early BAN420.
library(tidyverse)
library(readr)
library(tidymodels)
library(purrr)
# Read in the data from the file "growth.csv". This is a small data set
# covering income per capita and a few other variables in a cross section of
# countries. Make sure the formatting of the data is sensible. Drop the
# variables lbnppc_2014 and skole_p_1996. Thereafter, drop all observations with
# any missing observations. (You should have 94 obs. after this). Create a
# dependent variable "growth", defined as (lbnppc_1996-lbnppc_1960)/(1996-1960).
# This is the average annual growth rate of gdp per capita in the time period
# 1960 to 1996. Estimate a regression model with growth as a dependent variable,
# and initial income (lbnppc_1960) and the initial level of mining operations in
# the country (gruve_1960) as independent variables.
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
setwd("C:/Users/EirikHjalte/iCloudDrive/Høst 2021/BAN400/Lessons/many_models")
# Exercises for the many models lesson.
#
# 1. (Basic) Exercises 1-3 under 25.2.5 in R4DS
# 2. (Basic-ish) The Jackknife problem from early BAN420.
# 3. (Challenging) The all-combinations problem from early BAN420.
library(tidyverse)
library(readr)
library(tidymodels)
library(purrr)
# Read in the data from the file "growth.csv". This is a small data set
# covering income per capita and a few other variables in a cross section of
# countries. Make sure the formatting of the data is sensible. Drop the
# variables lbnppc_2014 and skole_p_1996. Thereafter, drop all observations with
# any missing observations. (You should have 94 obs. after this). Create a
# dependent variable "growth", defined as (lbnppc_1996-lbnppc_1960)/(1996-1960).
# This is the average annual growth rate of gdp per capita in the time period
# 1960 to 1996. Estimate a regression model with growth as a dependent variable,
# and initial income (lbnppc_1960) and the initial level of mining operations in
# the country (gruve_1960) as independent variables.
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
# Exercises for the many models lesson.
#
# 1. (Basic) Exercises 1-3 under 25.2.5 in R4DS
# 2. (Basic-ish) The Jackknife problem from early BAN420.
# 3. (Challenging) The all-combinations problem from early BAN420.
library(tidyverse)
library(readr)
library(tidymodels)
library(purrr)
# Read in the data from the file "growth.csv". This is a small data set
# covering income per capita and a few other variables in a cross section of
# countries. Make sure the formatting of the data is sensible. Drop the
# variables lbnppc_2014 and skole_p_1996. Thereafter, drop all observations with
# any missing observations. (You should have 94 obs. after this). Create a
# dependent variable "growth", defined as (lbnppc_1996-lbnppc_1960)/(1996-1960).
# This is the average annual growth rate of gdp per capita in the time period
# 1960 to 1996. Estimate a regression model with growth as a dependent variable,
# and initial income (lbnppc_1960) and the initial level of mining operations in
# the country (gruve_1960) as independent variables.
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
# Exercises for the many models lesson.
#
# 1. (Basic) Exercises 1-3 under 25.2.5 in R4DS
# 2. (Basic-ish) The Jackknife problem from early BAN420.
# 3. (Challenging) The all-combinations problem from early BAN420.
library(tidyverse)
library(readr)
library(tidymodels)
library(purrr)
# Read in the data from the file "growth.csv". This is a small data set
# covering income per capita and a few other variables in a cross section of
# countries. Make sure the formatting of the data is sensible. Drop the
# variables lbnppc_2014 and skole_p_1996. Thereafter, drop all observations with
# any missing observations. (You should have 94 obs. after this). Create a
# dependent variable "growth", defined as (lbnppc_1996-lbnppc_1960)/(1996-1960).
# This is the average annual growth rate of gdp per capita in the time period
# 1960 to 1996. Estimate a regression model with growth as a dependent variable,
# and initial income (lbnppc_1960) and the initial level of mining operations in
# the country (gruve_1960) as independent variables.
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
# Exercises for the many models lesson.
#
# 1. (Basic) Exercises 1-3 under 25.2.5 in R4DS
# 2. (Basic-ish) The Jackknife problem from early BAN420.
# 3. (Challenging) The all-combinations problem from early BAN420.
library(tidyverse)
library(readr)
library(tidymodels)
library(purrr)
# Read in the data from the file "growth.csv". This is a small data set
# covering income per capita and a few other variables in a cross section of
# countries. Make sure the formatting of the data is sensible. Drop the
# variables lbnppc_2014 and skole_p_1996. Thereafter, drop all observations with
# any missing observations. (You should have 94 obs. after this). Create a
# dependent variable "growth", defined as (lbnppc_1996-lbnppc_1960)/(1996-1960).
# This is the average annual growth rate of gdp per capita in the time period
# 1960 to 1996. Estimate a regression model with growth as a dependent variable,
# and initial income (lbnppc_1960) and the initial level of mining operations in
# the country (gruve_1960) as independent variables.
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
summary(lm(growth ~ lbnppc_1960 + gruve_1960, data = countries))
# Exercises for the many models lesson.
#
# 1. (Basic) Exercises 1-3 under 25.2.5 in R4DS
# 2. (Basic-ish) The Jackknife problem from early BAN420.
# 3. (Challenging) The all-combinations problem from early BAN420.
library(tidyverse)
library(readr)
library(tidymodels)
library(purrr)
# Read in the data from the file "growth.csv". This is a small data set
# covering income per capita and a few other variables in a cross section of
# countries. Make sure the formatting of the data is sensible. Drop the
# variables lbnppc_2014 and skole_p_1996. Thereafter, drop all observations with
# any missing observations. (You should have 94 obs. after this). Create a
# dependent variable "growth", defined as (lbnppc_1996-lbnppc_1960)/(1996-1960).
# This is the average annual growth rate of gdp per capita in the time period
# 1960 to 1996. Estimate a regression model with growth as a dependent variable,
# and initial income (lbnppc_1960) and the initial level of mining operations in
# the country (gruve_1960) as independent variables.
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
head(countries)
countries %>%
transmute(countries, - lbnppc_2014, - skole_p_1996)
view(countries)
is.na(countries)
is.na(max.print((countries))
print(is.na(max.print((countries)))
count(countries)
sum(countries)
sum(countries)
nrow(countries)
countries <-
read_delim("growth.csv", delim = ";") %>%
mutate(growth = (lbnppc_1996-lbnppc_1960)/(1996-1960)) %>%
select(-lbnppc_2014, -skole_p_1996) %>%
drop_na
view(countries)
lm([growth] ~ [lbnppc_1960, gruve_1960], countries )
lm(growth ~ lbnppc_1960, gruve_1960, countries )
view(countries)
lm(growth ~ lbnppc_1960, gruve_1960, data = countries )
lm(growth ~ lbnppc_1960 + gruve_1960, data = countries )
summary(lm(growth ~ lbnppc_1960 + gruve_1960, data = countries))
lm(growth ~ lbnppc_1960 + gruve_1960, data = countries ) #Regression model with growth as dependent, and initial income + gruve as independent variables.
#
# BAN400 - R PROGRAMMING FOR DATA SCIENCE
# LECTURE: MAKING MAPS
#
# In this lecture we will look at various techniques for making maps in R. We
# will start straight away by looking at some of the complicating factors that
# may arise when working with this kind of data, and then go into some simpler
# examples after that, to show that it does not always have to be all that
# complicated.
# EXAMPLE 1: The 1854 cholera outbreak ---------
#
# This example is presented in "Modern Data Science with R" by Baumer, Kaplan
# and Horton. The sp and rgdal-packages have been replaced by the sf-package.
library(mdsr)        # Companion R package to the book, containing data
library(sf)          # For spatial data, co-authoured by NHH Prof. Roger Bivand
library(ggmap)       # For downloading streetmaps as ggplot2-objects.
library(tidyverse)   # Data wrangling etc
plot(CholeraDeaths)  # Simple plot of the data
setwd("C:/Users/EirikHjalte/iCloudDrive/Høst 2021/BAN400/Lessons/maps")
#
# BAN400 - R PROGRAMMING FOR DATA SCIENCE
# LECTURE: MAKING MAPS
#
# In this lecture we will look at various techniques for making maps in R. We
# will start straight away by looking at some of the complicating factors that
# may arise when working with this kind of data, and then go into some simpler
# examples after that, to show that it does not always have to be all that
# complicated.
# EXAMPLE 1: The 1854 cholera outbreak ---------
#
# This example is presented in "Modern Data Science with R" by Baumer, Kaplan
# and Horton. The sp and rgdal-packages have been replaced by the sf-package.
library(mdsr)        # Companion R package to the book, containing data
library(sf)          # For spatial data, co-authoured by NHH Prof. Roger Bivand
library(ggmap)       # For downloading streetmaps as ggplot2-objects.
library(tidyverse)   # Data wrangling etc
plot(CholeraDeaths)  # Simple plot of the data
# Look at the data. The format is a little bit special. The simplest way to
# store this data would be to have three column in a data frame: count, lat and
# lon, but you see that we have a lot of different metadata here (some of which
# we will come back to), and that the spatial information, which in this case is
# the location of the adresses, is stored as a special geometrical object of the
# type POINT.
CholeraDeaths
#
# BAN400 - R PROGRAMMING FOR DATA SCIENCE
# LECTURE: MAKING MAPS
#
# In this lecture we will look at various techniques for making maps in R. We
# will start straight away by looking at some of the complicating factors that
# may arise when working with this kind of data, and then go into some simpler
# examples after that, to show that it does not always have to be all that
# complicated.
# EXAMPLE 1: The 1854 cholera outbreak ---------
#
# This example is presented in "Modern Data Science with R" by Baumer, Kaplan
# and Horton. The sp and rgdal-packages have been replaced by the sf-package.
library(mdsr)        # Companion R package to the book, containing data
library(sf)          # For spatial data, co-authoured by NHH Prof. Roger Bivand
library(ggmap)       # For downloading streetmaps as ggplot2-objects.
library(tidyverse)   # Data wrangling etc
plot(CholeraDeaths)  # Simple plot of the data
# Look at the data. The format is a little bit special. The simplest way to
# store this data would be to have three column in a data frame: count, lat and
# lon, but you see that we have a lot of different metadata here (some of which
# we will come back to), and that the spatial information, which in this case is
# the location of the adresses, is stored as a special geometrical object of the
# type POINT.
CholeraDeaths
